%!TEX root = ../THESIS.tex

\addcontentsline{toc}{chapter}{Introduction}
\chapter*{Introduction}

There are three main ingredients to this thesis:

\begin{itemize}
\item quantum computing,
\item natural language processing,
\item and category theory.
\end{itemize}

We will introduce them in order, followed by a summary of the thesis.

\section*{What are quantum computers good for?}
\epigraph{
Nature isn't classical, dammit, and if you want to make a simulation of
nature, you'd better make it quantum mechanical, and by golly it's a
wonderful problem, because it doesn't look so easy.
}{\textit{Simulating Physics with Computers}, Feynman (1981)}

Quantum computers harness the principles of quantum theory such as
superposition and entanglement to solve information-processing tasks. In
the space of 40 years, quantum computing has gone from theoretical
speculations to the implementation of machines that can solve problems
beyond what is possible with classical means. This section will sketch a
brief history of the field, and of its future challenges.

In 1980, Benioff~\cite{Benioff80} takes the abstract definition of a computer
and makes it physical: he designs a quantum mechanical system whose time evolution
encodes the computation steps of a given Turing machine. In
retrospective, this may be taken as the first proof that quantum
mechanics can simulate classical computers. The same year, Manin~\cite{Manin80} looks
at the opposite direction: he argues that it would take exponential time
for a classical computer to simulate a generic quantum system.
In the following years, Feynman~\cite{Feynman82,Feynman85} comes to the same
conclusion and suggests a way to simulate quantum mechanics much more
efficiently: building a quantum computer!

So what are quantum computers good for? Feynman's intuition gives us a
first, trivial answer: at least quantum computers can simulate quantum
mechanics efficiently. Deutsch~\cite{Deutsch85a} makes the question formal by
defining quantum Turing machines and quantum circuits.
Deutsch and Jozsa~\cite{DeutschJozsa92}
design the first quantum algorithm and prove that it solves
\emph{some} problem exponentially faster than any classical
\emph{deterministic}\footnote
{A classical \emph{randomized} algorithm solves the
problem in constant time with high probability.} algorithm.
Simon~\cite{Simon94} improves on their result by designing a problem that a
quantum computer can solve exponentially faster than any classical
algorithm. Deutsch-Jozsa and Simon relied on
oracles\footnote{An oracle is
a black box that allows a Turing machine to solve a certain problem in one step.}
and promises\footnote
{The input is promised to satisfy a certain property, which may be hard to check.}
and their problems have little practical use. However, they inspired Shor's
algorithm~\cite{Shor94a} for prime factorization and discrete logarithm. These
two problems are believed to require exponential time for a classical
computer and their hardness is at the basis of the public-key
cryptography schemes currently used on the internet.

In 1997, Grover provides another application for quantum computers:
``searching for a needle in a haystack''~\cite{Grover97}. Formally,
given a black box function $f : X \to \{0, 1\}$ and the promise that
there is a unique $x \in X$ with $f(x) = 1$, Grover's algorithm finds
$x$ in $O(\sqrt{|X|})$ steps, quadratically faster than the optimal
$O(|X|)$ classical algorithm. Grover's algorithm may be used to
brute-force symmetric cryptographic keys twice bigger than what is
possible classically~\cite{BernsteinEtAl09}. It can also be used to
obtain quadratic speedups for the exhaustive search involved in the
solution of NP-hard problems such as constraint satisfaction
~\cite{Ambainis04}. Independently, Benett et al.~\cite{BennettEtAl97}
prove that Grover's algorithm is in fact optimal, adding evidence to the
conjecture that quantum computers cannot solve these NP-hard problems in
polynomial time. The following year, Chuang et al.~\cite{ChuangEtAl98} give
the first experimental demonstration of a quantum algorithm, running
Grover's algorithm on two qubits.

Shor's and Grover's discovery of the first real-world applications
sparked a considerable interest in quantum computing. The core of these
two algorithms has then been abstracted away in terms of two
subroutines: phase estimation~\cite{Kitaev95} and amplitude
amplification~\cite{BrassardEtAl02}, respectively. Making use of both
these subroutines, the
HHL\footnote{Named after its discoverers Harrow, Hassidim and Lloyd.}
algorithm~\cite{HarrowEtAl09} tackles one of
the most ubiquitous problems in scientific computing: solving systems of linear
equations. Given a matrix $A \in \mathbb{R}^{n \times n}$ and a vector
$\vec{b} \in \mathbb{R}^{n}$, we want to find a vector $\vec{x}$ such that
$A \vec{x} = \vec{b}$. Under some assumptions on the sparsity and the condition
number of $A$, HHL finds (an approximation of) $x$ in time logarithmic in $n$
when a classical algorithm would take quadratic time simply to read the entries
of $A$. This initiated a new wave of enthusiasm for quantum computing
with the promise of exponential speedups for machine learning tasks such as
regression~\cite{WiebeEtAl12}, clustering~\cite{LloydEtAl13},
classification~\cite{RebentrostEtAl14}, dimensionality
reduction~\cite{LloydEtAl14a} and recommendation~\cite{KerenidisPrakash16}.
The narrative is appealing: machine learning is about finding patterns
in large amounts of data represented as high-dimensional vectors and tensors,
which is precisely what quantum computers are good at.

However, the exponential speedup of HHL comes with some caveats,
thoroughly analysed by Aaronson~\cite{Aaronson15}.
Two of these challenges are common to many quantum algorithms:
1) the efficient encoding of classical data into quantum states and
2) the efficient extraction of classical data via quantum measurements.
Indeed, what HHL really takes as input is not a vector $\vec{b}$ but
a quantum state $\ket{b} = \sum_{i=1}^n b_i \ket{i}$.
Either the input vector $\vec{b}$ has enough structure that we can describe
it with a simple, explicit formula.
This is the case for example in the calculation of electromagnetic scattering
cross-sections~\cite{CladerEtAl13}.
Or we need to assume that our classical data has been loaded onto a
quantum random-access memory (qRAM)~\cite{GiovannettiEtAl08}.
Not only is qRAM a daunting challenge from an engineering point of view,
in some cases it also involves too much error correction
for the state preparation to be efficient \cite{ArunachalamEtAl15}.
Symmetrically, the output of HHL is not the solution vector $\vec{x}$
itself but a quantum state $\ket{x}$ from which we can measure some observable
$\bra{x} M \ket{x}$. If preparing the state $\ket{b}$ requires a number of gates
exponential in the number of qubits, or if we need exponentially many
measurements of $\ket{x}$ to compute our classical output, then
the quantum speedup disappears.

Shor, Grover and HHL all assume \emph{fault-taulerant}
quantum computers~\cite{Shor96}.
Indeed, any machine we can build will be subject to noise when performing
quantum operations, errors are inevitable.
We need an error correcting code
that can correct these errors faster than they appear.
This is the content of the \emph{quantum threshold theorem}~\cite{AharonovBen-Or08}
which proves the possibility of fault-tolerant quantum computing
given physical error rates below a certain threshold.
One noteworthy example of such a quantum error correction scheme is Kitaev's
toric code~\cite{Kitaev03} and the general idea of topological quantum
computation~\cite{FreedmanEtAl03} which offers the long-term hope for a
quantum computer that is fault-tolerant \emph{by its physical nature}.
However this hope relies on the existence of quasi-particles called Majorana
zero-modes, which as of 2021 has yet to be experimentally demonstrated~\cite{Ball21}.

The road to large-scale fault-tolerant quantum computing will most likely be
a long one.
So in the meantime, what can we do with the noisy intermediate-scale quantum
machines we have available today, in the so-called NISQ era~\cite{Preskill18}?
Most answers involve a hybrid classical-quantum approach where a classical
algorithm is used to optimise the preparation of quantum states~\cite{McCleanEtAl16}.
Prominent examples include the quantum approximate optimization algorithm~\cite{FarhiEtAl14}
and the variational quantum eigensolver~\cite{PeruzzoEtAl14}.

parameterised quantum circuits as machine learning models \cite{BenedettiEtAl19}
barren plateaus~\cite{McCleanEtAl18}
One can help but notice the striking similarity with the vanishing gradient
problem for classical neural networks, formulated twenty years earlier~\cite{Hochreiter98}.
Barren plateaus are mitigated by clever initialisation~\cite{GrantEtAl19} or circuits with enough structure, e.g. convolution~\cite{PesahEtAl21}

Quantum advantage: the promise that quantum
computers will allow to solve problems that cannot be solved by
classical means in any reasonable time. First controversial claim by
Google with Sycamore's 53 qubits in 2019~\cite{AruteEtAl19},
challenged by IBM with a classical simulation~\cite{PednaultEtAl19},
then Jiuzhang's 76 photons~\cite{ZhongEtAl20a}.
The claim of quantum advantage has an asymptotic nature, thus
its definite proof will span a long period of time. However, quantum
advantage makes no claim of usefulness.

% \section*{Why should we make NLP quantum?}
%
% We have seen in the previous section that quantum computers cannot
% simply solve any problem faster: the problem needs to have some
% structure that a quantum computer can exploit. So why should we expect
% quantum computers to be any good at NLP?
%
% \section*{How can category theory help?}
%
% Via category theory!
%
% \section*{Summary of the thesis}
%
% The rest of this thesis will be structured as follows.
