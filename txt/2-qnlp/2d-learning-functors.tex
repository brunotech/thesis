%!TEX root = ../../THESIS.tex

\subsection{Variational quantum question answering}

The previous section introduced the idea of using gradient descent to learn DisCoCat models $F : \G \to \mathbf{Mat}_\C$ from data, we now discuss how to apply the same functorial learning approach in order to learn QNLP models $F : \G \to \mathbf{Circ}$.
There are two main challenges: 1) we need a way to load our model onto a quantum machine, i.e. encode word embeddings as parameterised quantum circuits, 2) we need a way to train the model, i.e. to compute the optimal parameters in some data-driven task.
As we mentioned in section~\ref{section:discocat-qnlp}, we could solve the first challenge by post-composing a classical model $F : \G \to \mathbf{Mat}_\C$ with any choice of encoding $\mathtt{load} : \mathbf{Mat}_\C \to \mathbf{Circ}$ to get a QNLP model $F \fcmp \mathtt{load} : \G \to \mathbf{Circ}$.
However, this would require circuits with depth exponential in the number of qubits, which are out of reach for the NISQ computers available today.
Instead of learning classical vectors of parameters that we then encode as circuits, we introduce a \emph{variational} quantum algorithm where we learn the parameters of quantum circuits directly.
As for the second challenge, we cannot hope to backpropagate gradients through a quantum circuit in the same way as for classical neural networks.
In this section we pick the easiest alternative: we treat our QNLP model as a black box and use a noisy optimisation algorithm such as the stochastic perturbation stochastic approximation (SPSA) of Spall~\cite{Spall98}.
In the next section, we will open the black box and introduce \emph{diagrammatic differentiation} in order to use the quantum circuits to compute their own gradients.

Our variational algorithm may be summarised in the following recipe for a parameterised circuit-valued functor.
\begin{enumerate}
\item Fix a pregroup grammar $G = (V, X, D, s)$ and use it to parse a dataset $\Omega \sub \coprod_{w_1 \dots w_n \in V^\star} \G(w_1 \dots w_n, s)$ of sentences then annotate them with truth values $Y : \Omega \to \R$.
\item Fix a hyper-parameter $F(x) \in \N$ for the number of qubits representing each basic type $x \in X$.
For simplicity, we will assume that $F(s) = 0$ so that sentences are represented as closed circuits.
\item Choose an \emph{ansatz} for each dictionary entry $(w, t) \in D$, i.e. a function $F_-(w, t) : \Theta_{(w, t)} \to \mathbf{Circ}(0, F(t))$ from some space of parameters $\Theta_{(w, t)}$ to the set of $F(t)$-qubit circuits.
Ideally, we want an ansatz that is shallow enough to run on NISQ machines but still hard to approximate classically, such as instantaneous quantum polynomials (IQP) of Shepherd and Bremner~\cite{ShepherdBremner09}.
A more practical choice is to use a \emph{hardware-efficient} ansatz such as that of Kandala et al.~\cite{KandalaEtAl17}, where we essentially squeeze as many parameters as possible out of whatever hardware we can get our hands on.
\end{enumerate}
As in the previous section, we can abstract these choices away into one big parameter space $\Theta = \prod_{(w, t) \in D} \Theta_{(w, t)}$ and a function $F_- : \Theta \to [\G, \mathbf{Circ}]$ from parameters to functors.
We can now use SPSA (or any noisy optimisation algorithm of our choice) to approximate the optimal parameters:
$$\theta^\star \s = \argmin_{\theta \in \Theta}
\lambda \lVert \theta \rVert + \sum_{f \in \Omega} \mathtt{loss}(\mathtt{eval}(F(f)), Y(f))$$
where $\mathtt{eval} : \mathbf{Circ}(0, 0) \to \R$ takes closed circuits and returns the result of evaluating them

We can now evaluate the optimal functor $F_{\theta^\star} : \G \to \mathbf{Circ}$ on unseen sentences, or equivalently answer Boolean questions.
This variational approach to question answering was first demonstrated in a classical simulation by Ma et al.~\cite{MaEtAl19} in the restricted case of knowledge graphs, i.e. subject-verb-object sentences.
Because the circuits were simulated classically it was possible to use standard gradient descent, with results comparable to the state-of-the-art.
The first NLP experiment on quantum hardware to appear in print was \cite{MeichanetzidisEtAl20}, where we used SPSA to learn a functor
