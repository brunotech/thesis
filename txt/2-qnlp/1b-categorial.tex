%!TEX root = ../../THESIS.tex

\subsection{From the Lambek calculus to DisCoCat models}

Even if we cannot get rid of the inherent ambiguity of natural language, we can still try to reduce the artificial ambiguity of our grammar formalism, i.e. the number of weakly equivalent grammars that generate the same language.

The \emph{categorial grammar} tradition may be summed up in a slogan: \emph{all the grammar is in the dictionary}~\cite{Preller07}.
Indeed, there is no need for language-specific production rules if the types of our grammar have enough structure, if we go from monoidal to closed categories.
In the \emph{Lambek calculus}~\cite{Lambek58}, a categorial grammar is defined as a tuple $G = (V, X, D, s)$ where:
\begin{itemize}
\item $V$ and $X$ are finite sets called the \emph{vocabulary} and the \emph{basic types} with $s \in X$ the \emph{sentence type},
\item $D \sub V \times T(X)$ is a finite set of \emph{dictionary entries} with $T(X) \supseteq X$ the set of formal expressions with $1, (x \otimes y), (x / y), (x \backslash y) \in T(X)$ for all $x, y \in T(X)$.
\end{itemize}
Equivalently, the Lambek grammar $G$ may be seen as a closed monoidal signature (as defined in section~\ref{subsection:closed}) with dictionary entries as boxes where the domain is a single word\footnote
{Note that the original calculus did not include a unit for the tensor product, here we follow the presentation given by Lambek~\cite{Lambek88} thirty years later.}.
In a \emph{basic categorial grammar}, also called an AB grammar after Ajdukiewicz~\cite{Ajdukiewicz35} and Bar-Hillel~\cite{Bar-Hillel54}, the dictionary is restricted to a closed signature, i.e. types are generated without the tensor product and unit.
The language of a categorial grammar $G$ is given by $L(G) = \{ w \in V^\star \ \vert \ \exists \ f : w \to s \in \G \}$ for $\G$ the free closed category generated by the dictionary.

More explicitly, a grammatical structure $f : w_1 \dots w_n \to s$ is given by a tensor of dictionary entries $(w_i, t_i) \in D$ followed by a closed diagram $t_1 \dots t_n \to s$ composed only of evaluation and currying.
Traditionally, these closed diagrams have been defined in terms of a \emph{sequent calculus} à la Gentzen, see Lambek~\cite{Lambek88} for a translation between the two definitions.
If we uncurry the identity on exponential types $x / y$ and $y \backslash x$ then curry them back the other way, we get the \emph{type raising} rules $x \to y \backslash (x / y)$ and $x \to (y \backslash x) / y$ which are analogous to the \emph{continuation-passing style} in functional programming~\cite{DeGroote01}.
Although it does not add affect the expressive power of the Lambek calculus, type raising allows \emph{incremental parsing} where sequences of words are processed strictly from left to right~\cite{Dowty88,Steedman91}, a feature which is well-motivated from a cognitive perspective.
In previous work, Shiebler, Sadrzadeh and the present author~\cite{ShieblerEtAl20} investigate incrementality in terms of a functor from grammars to automata.

\begin{python}
{\normalfont Implementation of categorial grammars as closed categories.}

\begin{minted}{python}
class Parsing(closed.Diagram, grammar.Parsing):
    def type_raise(x: closed.Ty, y: closed.Ty, left=True) -> Parsing:
        return Parsing.id(x >> y).uncurry().curry(left=False) if left\
            else Parsing.id(y << x).uncurry(left=False).curry()

class Ev(closed.Ev, Parsing): pass
class Word(grammar.Word, Parsing): pass
Ev.cast = Word.cast = Parsing.cast
\end{minted}
\end{python}

\begin{example}
We can take $X = \{ s, n, np \}$ and assign common noun the type $n$, determinants $(np \backslash n)$ and transitive verbs $((np / s) \backslash np)$.

\begin{minted}{python}
n, np, s = map(closed.Ty, ('n', 'np', 's'))
man, island = (Word(noun, n) for noun in ("man", "island"))
no, an = (Word(determinant, np << n) for determinant in ("no", "an"))
_is = Word("is", (np >> s) << np)

no_man_is_an_island = no @ man @ _is @ an @ island\
    >> Ev(np << n) @ ((np >> s) << np) @ Ev(np << n)\
    >> Parsing.type_raise(np, s) @ Ev((np >> s) << np)\
    >> Ev(s << (np >> s))

no_man_is_an_island.draw()
\end{minted}
\ctikzfig{img/nlp/no-man-is-an-island}
\end{example}

Bar-Hillel et al.~\cite{Bar-HillelEtAl60} showed that basic categorial grammars are strongly equivalent to context-free grammars in \emph{Greibach normal form}~\cite{Greibach65}, where every production has the form $x \to w y$ for a non-terminal $x \in X$, a word $w \in V$ and a string of non-terminals $y \in X^\star$.
Thus, their parsing problem can be solved in polynomial time.
Pentus~\cite{Pentus93} then showed that Lambek grammars are weakly equivalent to CFGs as well, although their parsing problem is $\mathtt{NP}$-complete~\cite{Pentus06}.
This means that unless $\mathtt{P} = \mathtt{NP}$ there are Lambek grammars for which the smallest weakly equivalent CFG will have exponential size.
Many extensions of the Lambek calculus have been introduced to go beyond its context-free limitation and give a more fine-grained description of syntactic phenomena, see Moortgat~\cite{Moortgat14} for a survey.
Additional unary operators called \emph{modalities}\footnote
{There is no consensus on what the definition of modality should be: ``Ask three modal logicians what modal logic is, and you are likely to get at least three different answers''~\cite{BlackburnEtAl02}
In most cases however, modalities are \emph{comonads}~\cite{CirsteaEtAl11}.} allow to break away from the planarity and linearity of closed diagrams, introducing rules for swaps and comonoids in a controlled way to model phenomena such as \emph{parasitic gaps}, \emph{ellipsis} and \emph{anaphora}.
See McPheat et al.~\cite{McPheatEtAl21} where the author and collaborators introduce a diagrammatic syntax and functorial semantics for such modalities.
The \emph{combinatory categorial grammars} (CCGs) of Steedman~\cite{Steedman87,Steedman00} take a different approach inspired by the combinatory logic of Schönfinkel~\cite{Schonfinkel24} and Curry~\cite{Curry30}, a variable-free predecessor to the lambda-calculus.
In particular, CCGs include \emph{crossed composition} rules which make them mildly context-sensitive, see Kartsaklis and Yeung~\cite{YeungKartsaklis21} for their implementation in DisCoPy.

A key feature of categorial grammars as free closed categories, is that we can define their semantics as functors into any closed category: once the image of dictionary entries is defined, the image of any grammatical structure is fixed.
Montague~\cite{Montague70a,Montague70,Montague73} introduced the idea of semantics as a homomorphism from syntax to logic, i.e. as a closed functor $F : \G \to F^{CC}(\Sigma)$ from a categorial grammar into a free cartesian closed category with logical connectives and predicates as boxes.
Although Montague himself did not care much about syntax~\footnote
{``I fail to see any great interest in syntax except as a preliminary to semantics.''~\cite{Montague70a}},
his method provides a general recipe to interpret any categorial grammar in terms of lambda expressions.
From a computational perspective however, Montague semantics is too expressive: as we mentioned in section~\ref{subsection:closed}, the word problem for free cartesian closed categories, or equivalently the normalisation of simply-typed lambda terms, is not elementary recursive.
The \emph{Entscheidungsproblem} of Hilbert and Ackermann~\cite{HilbertAckerman28} is to decide, given a first-order logic formula, whether it is valid (i.e. true in every interpretation).
Church~\cite{Church36} proved this is undecidable, thus even if we manage to translate sentences as first-order logic formulae (which could take non-elementary time) checking if a given sentence is valid (or if two sentences are equivalent) is also undecidable.
Intuitively, we can reduce Turing's halting problem to the validity of the sentence ``the machine halts''.
The \emph{model checking problem} is to decide whether a formula is valid in a given finite model, it is $\mathtt{PSPACE}$-complete in the size of the formula~\cite[Theorem~4.3]{Gradel02} and in $\mathtt{L}$ (logarithmic space) if the formula is fixed~\cite[Corollary~4.5]{Gradel02}.

\begin{example}
We can interpret natural language as arbitrary Python code by applying a closed functor $\G \to \mathbf{Pyth}$.

\begin{minted}{python}
x, y = map(Ty, "xy")
program, runs = Word("program", x), Word("runs", x >> y)
program_runs = program @ runs >> Ev(x >> y)

F = closed.Functor(
    dom=Category(Ty, Parsing), cod=Category(tuple[type, ...], Function),
    ob={x: int, y: int}, ar={program: lambda: 42, runs: lambda: lambda n: n * 10})
assert F(program_runs)() == 420
\end{minted}
\end{example}

\begin{example}
We can implement Montague semantics as a closed functor $\G \to \mathbf{Pyth}$ which sends the sentence type to \py{Formula}, the implementation of diagrammatic first-order logic à la Peirce given in example~\ref{example:monoidal-formula}.
Montague~\cite{Montague73} defines common nouns and intransitive verb phrases as functions from terms to formulae, in diagrammatic logic the same role is played by states and effects, i.e. open formulae with one open wire \py{x} in the codomain and domain respectively.
Montague's noun phrases are functions from intransitive verb phrase to sentence, in our setting they are given by functions from open to closed formulae.
The transitive verb ``is'' must take two such functions \py{P} and \py{Q} as input and return a closed formula, the only thing we can do is apply \py{P} to the identity diagram on \py{x} to get a state, before feeding the dagger of the result to \py{Q}.

\begin{minted}{python}
x = Ty('x')

Montague = closed.Functor(
    dom=Category(Ty, Parsing), cod=Category(tuple[type, ...], Function),
    ob={s: Formula, n: Formula, np: exp(Formula, Formula)},
    ar={no: lambda: lambda state: lambda effect: (state >> effect).bubble(),
        man: lambda: Predicate("man", x),
        _is: lambda: lambda P: lambda Q: Q(P(Formula.id(x)).dagger()),
        an: lambda: lambda state: lambda effect: state >> effect,
        island: lambda: Predicate("island", x)})
Montague(no_man_is_an_island)().draw()
\end{minted}

\ctikzfig{img/nlp/peirce-montague}

\begin{minted}{python}
size = {x: 2}

for mans, islands in itertools.product(*2 * [itertools.product(*size[x] * [[0, 1]])]):
    F = model(size, {Predicate("man", x): mans, Predicate("island", x): islands})
    assert F(Montague(no_man_is_an_island)()) == not any(
        F(Predicate("man", x))[i] and F(Predicate("island", x))[i] for i in range(size[x]))
\end{minted}
\end{example}

Returning to linguistics half a century after his seminal \emph{Mathematics of sentence structure}, Lambek~\cite{Lambek99,Lambek01,Lambek08} introduced \emph{pregroup grammars} as a simplification of his original calculus replacing closed categories by rigid categories.
That is, the dictionary of a pregroup grammar $G = (V, X, D, s)$ has the shape $D \sub V \times (X \times \Z)^\star$, it assigns words to lists of iterated adjoints of basic types.
Again, the language of $G$ is defined as $L(G) = \{ w \in V^\star \ \vert \ \exists \ f : w \to s \in \G \}$ where now $\G$ is the free rigid category generated by the dictionary.
Equivalently, a sentence $w = w_1 \dots w_n \in V^\star$ is grammatical if there are dictionary entries $(w_i, t_i) \in D$ such that $t_1 \dots t_n \leq s$ holds in the free pregroup, i.e. the preorder collapse of $\G$.
In fact, Lambek first defined his pregroup grammars in terms of partial orders (i.e. preorders with antisymmetry) then Preller and he~\cite{PrellerLambek07} reformulated them in terms of free compact 2-categories (i.e. rigid categories with colours) so that they could account for ambiguity.
Moving from preorders to free categories also allows to define pregroup semantics as functors, indeed a functor with a preorder as domain is required to map all the parsings of an ambiguous sentence to the same meaning.
Even worse, a monoidal functor with a pregroup as domain has to obey the equation $F(x) = F(x \otimes x^l \otimes x)$ for all types $x \in X$, which makes the functor trivial in categories of interest such as $\mathbf{Set}$ or $\mathbf{Mat}_\S$.

Every rigid category is also closed, thus for any Lambek grammar $G$ we can construct a pregroup grammar $G'$ and a closed functor $\G \to \G'$ which sends categorial types $x / y$ and $x \backslash y$ to pregroup types $x^r y$ and $x y^l$.
In general this functor need not be faithful: it maps both $(x \otimes y) / z$ and $x \otimes (y / z)$ to the same pregroup type $x y z^l$.
Although they cannot be strongly equivalent to Lambek grammars, Buszkowski~\cite{Buszkowski01} proved that pregroup grammars are context-free, hence they are still weakly-equivalent.
As for the complexity of their parsing problem, Lambek~\cite{Lambek99} first showed it was decidable with the following \emph{switching lemma}: any pregroup inequality $x \leq z$ can be factored into $x \leq y \leq z$ where $x \leq y$ using only cups then $y \leq z$ using only caps.
The proof is essentially given by the snake removal algorithm of listing~\ref{listing:snake-removal} applied to rigid diagrams with only cups and caps: the resulting normal form can be shown to have all cups preceding the caps.
As a corollary, if there is a grammatical structure $f : w_1 \dots w_n \to s$ then there is one using only dictionary entries and cups which we can find by brute force search, Oehrle~\cite{Oehrle04} showed that pregroup parsing can in fact be solved in cubic time.
Preller~\cite{Preller07a} gave sufficient conditions on the dictionary for unambiguous pregroup grammars to be parsed in linear time, the algorithm was later improved and implemented in DisCoPy by Rizzo~\cite{Rizzo21}.
Multiple extensions of pregroup grammars have been proposed to go beyond context-free languages including taking products of free pregroups~\cite[Section~28]{Lambek08}, grammars with infinite dictionaries~\cite{Preller10} or a notion of buffer~\cite{GenkinEtAl10}.

One can define the semantics of a pregroup grammar $G$ as a monoidal functor $F : \G \to C$ into any rigid category $C$, in the \emph{categorical compositional distributional} (DisCoCat) models of Clark, Coecke and Sadrzadeh~\cite{ClarkEtAl08,ClarkEtAl10} one takes $C = \mathbf{Mat}_\S$ the category of matrices.
More explicitly, a DisCoCat model $F : \G \to \mathbf{Mat}_\S$ is defined by a dimension $F(x) \in \N$ for every basic type $x \in X$ and a vector $F(w, t) : 1 \to F(t)$ for every dictionary entry $(w, t) \in D$.
It then defines the semantics of a grammatical sentence $f : w_1 \dots w_n \to s$ as the contraction of a tensor network where the nodes are dictionary entries and the edges are given by the cups.
Note that the two seminal articles~\cite{ClarkEtAl08,ClarkEtAl10} do not mention rigid categories and stick with the definition of pregroup grammars in terms of partial orders.
Unable to define non-trivial functors $P \to \mathbf{Mat}_\S$ for $P$ the free pregroup generated by $G$, i.e. the preorder collapse of $\G$, they resort to defining DisCoCat in terms of a product category $\mathbf{Mat}_\S \times P$.
Although they do not say it explicitly, they in fact worked with a subcategory of $\G \times \mathbf{Mat}_\S$ called the \emph{Grothendieck construction} on a monoidal functor $F : \G \to \mathbf{Mat}_\S$, see Bradley et al.~\cite{BradleyEtAl18} for an application of this observation to model translation and language evolution in DisCoCat.

DisCoCat models came out of a quest to accommodate the \emph{compositional} approach to NLP which focused on grammatical structure and the \emph{distributional} apprach that represented words as vectors extracted from text data.
A key feature of this approach compared to its predecessor is that DisCoCat models define a similarity measure between any two expressions of the same type, even though they do not have the same grammatical structure.
Indeed, any two pregroup diagrams $f, g : w_1 \dots w_n \to x$ will be mapped to vectors of dimension $n = F(x)$ such that the inner product $\langle F(f) \vert F(g) \rangle$ yields a measure of their similarity.
We can then use standard machine learning techniques to solve problems such as classification, an approach that has received some empirical support on small-scale datasets~\cite{GrefenstetteSadrzadeh11}.
The name of DisCoPy stands for \emph{Distributional Compositional Python}, indeed it was first meant as an implementation of DisCoCat models before it turned into an implementation of monoidal functors in general.

\begin{python}
{\normalfont Implementation of pregroup grammars as rigid categories.}

\begin{minted}{python}
class Parsing(rigid.Diagram, categorial.Parsing): pass
class Cup(rigid.Cup, Parsing): pass
class Word(categorial.Word, Parsing): pass
Cup.cast = Word.cast = Parsing.cast
\end{minted}
\end{python}

\begin{example}\label{example:alice-loves-bob}
Compting the meaning of ``Alice loves Bob'' was the first example in DisCoPy's documentation.

\begin{minted}{python}
s, n = rigid.Ty('s'), rigid.Ty('n')
Alice, loves, Bob = Word('Alice', n), Word('loves', n.r @ s @ n.l), Word('Bob', n)
sentence = Alice @ loves @ Bob >> Cup(n, n.r) @ s @ Cup(n.l, n)

sentence.draw()
\end{minted}

\ctikzfig{img/nlp/alice-loves-bob}

\begin{minted}{python}
F = rigid.Functor(
    dom=Category(rigid.Ty, Parsing), cod=Category(tuple[int, ...], Tensor),
    ob={s: 1, n: 2}, ar={Alice: [[1, 0]], loves: [[0, 1], [1, 0]], Bob: [[0, 1]]})

assert F(sentence)
\end{minted}
\end{example}
