%!TEX root = ../../THESIS.tex

\subsection{From Lambek to pregroup grammars}

Even if we cannot get rid of the inherent ambiguity of natural language, we can still try to reduce the artificial ambiguity of our grammar formalism, i.e. the number of weakly equivalent grammars that generate the same language.

The \emph{categorial grammar} tradition may be summed up in a slogan: \emph{all the grammar is in the dictionary}~\cite{Preller07}.
Indeed, there is no need for language-specific production rules if the types of our grammar have enough structure, if we go from monoidal to closed categories.
In the \emph{Lambek calculus}~\cite{Lambek58}, a categorial grammar is defined as a tuple $G = (V, X, R, s)$ where:
\begin{itemize}
\item $V$ and $X$ are finite sets called the \emph{vocabulary} and the \emph{basic types} with $s \in X$ the \emph{sentence type},
\item $R \sub V \times T(X)$ is a finite set of \emph{dictionary entries} with $T(X) \supseteq X$ the set of formal expressions with $1, (x \otimes y), (x / y), (x \backslash y) \in T(X)$ for all $x, y \in T(X)$.
\end{itemize}
Equivalently, the Lambek grammar $G$ may be seen as a closed monoidal signature (as defined in section~\ref{subsection:closed}) with dictionary entries as boxes where the domain is a single word\footnote
{Note that the original calculus did not include a unit for the tensor product, here we follow the presentation given by Lambek~\cite{Lambek88} thirty years later.}.
In a \emph{basic categorial grammar}, also called an AB grammar after Ajdukiewicz~\cite{Ajdukiewicz35} and Bar-Hillel~\cite{Bar-Hillel54}, the dictionary is restricted to a closed signature, i.e. types are generated without the tensor product and unit.
The language of a categorial grammar $G$ is given by $L(G) = \{ w \in V^\star \ \vert \ \exists \ f : w \to s \in \G \}$ for $\G$ the free closed category generated by the dictionary.

More explicitly, a grammatical structure $f : w_1 \dots w_n \to s$ is given by a tensor of dictionary entries $(w_i, t_i) \in R$ followed by a closed diagram $t_1 \dots t_n \to s$ composed only of evaluation and currying.
Traditionally, these closed diagrams have been defined in terms of a \emph{sequent calculus} à la Gentzen, see Lambek~\cite{Lambek88} for a translation between the two definitions.
If we uncurry the identity on exponential types $x / y$ and $y \backslash x$ then curry them back the other way, we get the \emph{type raising} rules $x \to y \backslash (x / y)$ and $x \to (y \backslash x) / y$ which are analogous to the \emph{continuation-passing style} in functional programming~\cite{DeGroote01}.
Although it does not add affect the expressive power of the Lambek calculus, type raising allows \emph{incremental parsing} where sequences of words are processed strictly from left to right~\cite{Dowty88,Steedman91}, a feature which is well-motivated from a cognitive perspective.
In previous work, Shiebler, Sadrzadeh and the present author~\cite{ShieblerEtAl20} investigate incrementality in terms of a functor from grammars to automata.

\begin{python}
{\normalfont Implementation of categorial grammars as closed categories.}

\begin{minted}{python}
class Parsing(closed.Diagram, cfg.Parsing):
    def type_raise(x: closed.Ty, y: closed.Ty, left=True) -> Parsing:
        return Parsing.id(x >> y).uncurry().curry(left=False) if left\
            else Parsing.id(y << x).uncurry(left=False).curry()

class Ev(closed.Ev, Parsing): pass
class Word(cfg.Word, Parsing): pass
Ev.cast = Word.cast = Parsing.cast
\end{minted}
\end{python}

\begin{example}
We can take $X = \{ s, n, np \}$ and assign common noun the type $n$, determinants $(np \backslash n)$ and transitive verbs $((np / s) \backslash np)$.

\begin{minted}{python}
n, np, s = map(closed.Ty, ('n', 'np', 's'))
man, island = (Word(noun, n) for noun in ("man", "island"))
no, an = (Word(determinant, np << n) for determinant in ("no", "an"))
_is = Word("is", (np >> s) << np)

no_man_is_an_island = no @ man @ _is @ an @ island\
    >> Ev(np << n) @ ((np >> s) << np) @ Ev(np << n)\
    >> Parsing.type_raise(np, s) @ Ev((np >> s) << np)\
    >> Ev(s << (np >> s))

no_man_is_an_island.draw()
\end{minted}
\ctikzfig{img/nlp/no-man-is-an-island}
\end{example}

Bar-Hillel et al.~\cite{Bar-HillelEtAl60} showed that basic categorial grammars are strongly equivalent to context-free grammars in \emph{Greibach normal form}~\cite{Greibach65}, where every production has the form $x \to w y$ for a non-terminal $x \in X$, a word $w \in V$ and a string of non-terminals $y \in X^\star$.
Thus, their parsing problem can be solved in polynomial time.
Pentus~\cite{Pentus93} then showed that Lambek grammars are weakly equivalent to CFGs as well, although their parsing problem is $\mathtt{NP}$-complete~\cite{Pentus06}.
This means that unless $\mathtt{P} = \mathtt{NP}$ there are Lambek grammars for which the smallest weakly equivalent CFG will have exponential size.
Many extensions of the Lambek calculus have been introduced to go beyond its context-free limitation and give a more fine-grained description of syntactic phenomena, see Moortgat~\cite{Moortgat14} for a survey.
Additional unary operators called \emph{modalities}\footnote
{There is no consensus on what the definition of modality should be: ``Ask three modal logicians what modal logic is, and you are likely to get at least three different answers''~\cite{BlackburnEtAl02}
In most cases however, modalities are \emph{comonads}~\cite{CirsteaEtAl11}.} allow to break away from the planarity and linearity of closed diagrams, introducing rules for swaps and comonoids in a controlled way to model phenomena such as \emph{parasitic gaps}, \emph{ellipsis} and \emph{anaphora}.
See McPheat et al.~\cite{McPheatEtAl21} where the author and collaborators introduce a diagrammatic syntax and functorial semantics for such modalities.
The \emph{combinatory categorial grammars} (CCGs) of Steedman~\cite{Steedman87,Steedman00} take a different approach inspired by the combinatory logic of Schönfinkel~\cite{Schonfinkel24} and Curry~\cite{Curry30}, a variable-free predecessor to the lambda-calculus.
In particular, CCGs include \emph{crossed composition} rules which make them mildly context-sensitive, see Kartsaklis and Yeung~\cite{YeungKartsaklis21} for their implementation in DisCoPy.

A key feature of categorial grammars as free closed categories, is that we can define their semantics as functors into any closed category: once the image of dictionary entries is defined, the image of any grammatical structure is fixed.
Montague~\cite{Montague70a,Montague70,Montague73} introduced the idea of semantics as a homomorphism from syntax to logic, i.e. as closed functors $F : \G \to F^{CC}(\Sigma)$ from a categorial grammar into a free cartesian closed category with logical connectives and predicates as boxes.
Although Montague himself did not care much about syntax~\footnote
{``I fail to see any great interest in syntax except as a preliminary to semantics.''~\cite{Montague70a}},
his method provides a general recipe to interpret any categorial grammar in terms of lambda expressions.

From a computational perspective however, Montague semantics is too expressive: as we mentioned in section~\ref{subsection:closed}, the word problem for free cartesian closed categories, or equivalently the normalisation of simply-typed lambda terms, is not elementary recursive.
The \emph{Entscheidungsproblem} of Hilbert and Ackermann~\cite{HilbertAckerman28} is to decide, given a first-order logic formula, whether it is valid (i.e. true in every interpretation).
Church~\cite{Church36} proved this is undecidable, thus even if we manage to translate sentences as first-order logic formulae (which could take non-elementary time) checking if a given sentence is valid (or if two sentences are equivalent) is also undecidable.
Intuitively, we can reduce Turing's halting problem to the validity of the sentence ``the machine halts''.
The \emph{model checking problem} is to decide whether a formula is valid in a given finite model, it is $\mathtt{PSPACE}$-complete in the size of the formula~\cite[Theorem~4.3]{Gradel02} and in $\mathtt{L}$ (logarithmic space) if the formula is fixed~\cite[Corollary~4.5]{Gradel02}.

\begin{example}
We can interpret natural language as arbitrary Python code by applying a closed functor $\G \to \mathbf{Pyth}$.

\begin{minted}{python}
x, y = map(Ty, "xy")
program, runs = Word("program", x), Word("runs", x >> y)
program_runs = program @ runs >> Ev(x >> y)

F = closed.Functor(
    dom=Category(Ty, Parsing), cod=Category(list[type], Function),
    ob={x: int, y: int}, ar={program: lambda: 42, runs: lambda: lambda n: n * 10})
assert F(program_runs)() == 420
\end{minted}
\end{example}

\begin{example}
We can implement Montague semantics as a closed functor $\G \to \mathbf{Pyth}$ which sends the sentence type to \py{Formula}, the implementation of diagrammatic first-order logic à la Peirce given in example~\ref{example:monoidal-formula}.
Montague~\cite{Montague73} defines common nouns and intransitive verb phrases as functions from terms to formulae, in diagrammatic logic the same role is played by states and effects, i.e. open formulae with one open wire \py{x} in the codomain and domain respectively.
Montague's noun phrases are functions from intransitive verb phrase to sentence, in our setting they are given by functions from open to closed formulae.
The transitive verb ``is'' must take two such functions \py{P} and \py{Q} as input and return a closed formula, the only thing we can do is apply \py{P} to the identity diagram on \py{x} to get a state, before feeding the dagger of the result to \py{Q}.

\begin{minted}{python}
x = Ty('x')

Montague = closed.Functor(
    dom=Category(Ty, Parsing), cod=Category(list[type], Function),
    ob={s: Formula, n: Formula, np: exp(Formula, Formula)},
    ar={no: lambda: lambda state: lambda effect: (state >> effect).bubble(),
        man: lambda: Predicate("man", x),
        _is: lambda: lambda P: lambda Q: Q(P(Formula.id(x)).dagger()),
        an: lambda: lambda state: lambda effect: state >> effect,
        island: lambda: Predicate("island", x)})
Montague(no_man_is_an_island)().draw()
\end{minted}

\ctikzfig{img/nlp/peirce-montague}

\begin{minted}{python}
size = {x: 2}

for mans, islands in itertools.product(*2 * [itertools.product(*size[x] * [[0, 1]])]):
    F = model(size, {Predicate("man", x): mans, Predicate("island", x): islands})
    assert F(Montague(no_man_is_an_island)()) == not any(
        F(Predicate("man", x))[i] and F(Predicate("island", x))[i] for i in range(size[x]))
\end{minted}
\end{example}

Returning to linguistics half a century after his seminal \emph{Mathematics of sentence structure}, Lambek~\cite{Lambek99,Lambek01,Lambek08} introduced \emph{pregroup grammars} as a simplification of his original calculus replacing closed categories by rigid categories.
That is, the dictionary of a pregroup grammar $G = (V, X, R, s)$ has the shape $R \sub V \times (X \times \Z)^\star$, it assigns words to lists of iterated adjoints of basic types.
Again, the language of $G$ is defined as $L(G) = \{ w \in V^\star \ \vert \ \exists \ f : w \to s \in \G \}$ where now $\G$ is the free rigid category generated by the dictionary.
Equivalently, a sentence $w = w_1 \dots w_n \in V^\star$ is grammatical if there are dictionary entries $(w_i, t_i) \in R$ such that $t_1 \dots t_n \leq s$ holds in the free pregroup, i.e. the preorder collapse of $\G$.
In fact, Lambek first defined his pregroup grammars in terms of partial orders (i.e. preorders with antisymmetry) then Preller and he~\cite{PrellerLambek07} reformulated them in terms of free compact 2-categories (i.e. rigid categories with colours) so that they could account for ambiguity.

Every rigid category is also closed, thus for any Lambek grammar $G$ we can construct a pregroup grammar $G'$ and a closed functor $\G \to \G'$ which sends categorial types $x / y$ and $x \backslash y$ to pregroup types $x^r y$ and $x y^l$.
In general this functor need not be faithful: it maps both $(x \otimes y) / z$ and $x \otimes (y / z)$ to the same pregroup type $x y z^l$.
Although they cannot be strongly equivalent to Lambek grammars, Buszkowski~\cite{Buszkowski01} proved that pregroup grammars are context-free, hence they are still weakly-equivalent.
As for the complexity of their parsing problem, Lambek~\cite{Lambek99} first showed it was decidable with the following \emph{switching lemma}: any pregroup inequality $x \leq z$ can be factored into $x \leq y \leq z$ where $x \leq y$ using only cups then $y \leq z$ using only caps.
The proof is essentially given by the snake removal algorithm of listing~\ref{listing:snake-removal} applied to rigid diagrams with only cups and caps: the resulting normal form can be shown to have all cups preceding the caps.
As a corollary, if there is a grammatical structure $f : w_1 \dots w_n \to s$ then there is one using only dictionary entries and cups which we can find by brute force search, Oehrle~\cite{Oehrle04} showed that pregroup parsing can in fact be solved in cubic time.
Preller~\cite{Preller07a} gave sufficient conditions on the dictionary for unambiguous pregroup grammars to be parsed in linear time, the algorithm was later improved and implemented in DisCoPy by Rizzo~\cite{Rizzo21}.
Multiple extensions of pregroup grammars have been proposed to go beyond context-free languages including taking products of free pregroups~\cite[Section~28]{Lambek08}, grammars with infinite dictionaries~\cite{Preller10} or a notion of buffer~\cite{GenkinEtAl10}.

One can define the semantics of a pregroup grammar $G$ as a monoidal functor $F : \G \to C$ into any rigid category $C$, in the \emph{categorical compositional distributional} (DisCoCat) models of Clark, Coecke and Sadrzadeh~\cite{ClarkEtAl08,ClarkEtAl10} one takes $C = \mathbf{Mat}_\S$ the category of matrices.
More explicitly, a DisCoCat model $F : \G \to \mathbf{Mat}_\S$ is defined by a dimension $F(x) \in \N$ for every basic type $x \in X$ and a vector $F(w, t) : 1 \to F(t)$ for every dictionary entry $(w, t) \in R$.
It then defines the semantics of a grammatical sentence $f : w_1 \dots w_n \to s$ as the contraction of a tensor network where the nodes are dictionary entries and the edges are given by the cups.
DisCoCat models came out of a quest to accommodate the \emph{compositional} approach to NLP which focused on grammatical structure and the \emph{distributional} apprach that represented words as vectors extracted from text data.
DisCoPy was first meant as an implementation of DisCoCat models before it turned into an implementation of monoidal functors in general.

\begin{python}
{\normalfont Implementation of pregroup grammars as rigid categories.}

\begin{minted}{python}
class Parsing(rigid.Diagram, categorial.Parsing): pass
class Cup(rigid.Cup, Parsing): pass
class Word(categorial.Word, Parsing): pass
Cup.cast = Word.cast = Parsing.cast
\end{minted}
\end{python}

\begin{example}
This was the first example in DisCoPy's documentation.

\begin{minted}{python}
s, n = Ty('s'), Ty('n')
Alice, loves, Bob = Word('Alice', n), Word('loves', n.r @ s @ n.l), Word('Bob', n)
sentence = Alice @ loves @ Bob >> Cup(n, n.r) @ s @ Cup(n.l, n)

sentence.draw()
\end{minted}

\ctikzfig{img/nlp/alice-loves-bob}

\begin{minted}{python}
F = rigid.Functor(
    dom=Category(Ty, Parsing), cod=Category(list[int], Tensor),
    ob={s: 1, n: 2}, ar={Alice: [[1, 0]], loves: [[0, 1], [1, 0]], Bob: [[0, 1]]})

assert F(sentence)
\end{minted}
\end{example}

While the meaning of \emph{lexical words} (also called \emph{content} words) such as nouns and verbs are extracted from text data, DisCoCat models allow to encode \emph{functional words} such as pronouns and conjunctions in terms of the Frobenius algebras, a.k.a. spiders, we discussed in section~\ref{subsection:hypergraph}.
This \emph{Frobenius anatomy of word meanings} was first applied to relative pronouns~\cite{SadrzadehEtAl13,SadrzadehEtAl14}, then to coordination~\cite{Kartsaklis16} as well as intonation~\cite{KartsaklisSadrzadeh15}.
In previous work with Coecke, de Felice and Marsden~\cite{CoeckeEtAl18a} as well as in a subsequent dissertation~\cite{Toumi18a}, we proposed the use of spiders to model \emph{anaphora}, expressions such as personal pronouns whose meaning depends on another expression in context, connecting the diagrams for sentences together into a diagram for \emph{discourse}.
This proposal came with an algorithm for constructing a \emph{relational database} from any such discourse diagram and for translating the diagrams of questions into database queries.

This discourse-to-database translation was refined in later work with de Felice and Meichanetzidis~\cite{FeliceEtAl19} where we defined the \emph{functorial question answering} problem as the application of a given DisCoCat model to a question diagram.
In the case of Boolean-valued models $F : \G \to \mathbf{Mat}_\S$, we proved that this question-answering problem is in fact equivalent to \emph{conjunctive query evaluation}, which is $\mathtt{NP}$-complete by a celebrated theorem of Chandra and Merlin~\cite{ChandraMerlin77}.
Conjunctive queries can be defined as Peircean diagrams with no bubbles, i.e. only spiders and predicate boxes, or equivalently as the first-order logic formulae with existentials and conjunction but no negation, see Bonchi et. al for a diagrammatic treatment~\cite{BonchiEtAl18}.
The Chandra-Merlin theorem is based on the construction of a \emph{canonical model} for the given query, i.e. a canonical functor given a diagram, then reducing evaluation to the problem of graph homomorphism between models.
Once translated in terms of Boolean DisCoCat models, the same result implies that question-answering (i.e. the application of a functor to a diagram) is equivalent to the \emph{entailment problem}: given two sentences, does the truth of one imply that the other?

DisCoCat models with spiders are unsatisfying in two opposite ways: they are not expressive enough to encode negation\footnote
{Note that the results of \cite{FeliceEtAl19} assume that the sentence type is mapped to the unit, i.e. the meaning of a sentence is a scalar in $\B$.
Preller~\cite{Preller14a,Preller14} takes an alternative, four-valued approach to first-order logic with pregroups where the sentence type is given dimension $2$: a sentence is either true, false, neither or both.}
but if allow arbitrary anaphora, they are already too expressive to be computed efficiently.
The first dissatisfaction cannot be avoided if we want our models to be tractable: adding negation to conjunctive queries will generate all of first-order logic, for which question-answering (i.e. model checking) is $\mathtt{PSPACE}$-complete and entailment (i.e. the Entscheidungsproblem) undecidable.
We may get around the second dissatisfaction by adding restrictions on anaphoric expressions, for example requiring that the corresponding query has \emph{bounded tree-width}~\cite{ChekuriRajaraman00} ensures that question answering is solvable in polynomial time.
This restriction may be motivated in terms of \emph{bounded short-term memory}: a query with tree-width $k$ corresponds to a first-order logic with $k$ variables, each of which may be bound multiple times.
We refer to Abramsky and Shah~\cite{AbramskyShah18} for a comonadic approach to such resource bounds.

If we go from Booleans to natural numbers, we get a \emph{counting problem}: we want to know not only whether but \emph{how many} answers a question has in a given model.
This answer-counting problem is complete for $\mathtt{\#P}$, the generalisation of $\mathtt{NP}$ from decision to counting problems.
By extension, evaluating DisCoCat models over the real or complex numbers (with finite precision) is also $\mathtt{\#P}$-complete.
The closest decision complexity class is $\mathtt{PP}$, also called \emph{Majority-P}, the class of problems solvable in probabilistic polynomial time with no error bounds, which amounts to computing the most significant digit of a $\mathtt{\#P}$ problem.
If we write $\mathtt{P}^\mathtt{X}$ for the class of problems solvable in polynomial time with access to an $\mathtt{X}$-oracle solving any problem of $\mathtt{X}$ in one step, we can use a binary search to prove $\mathtt{P}^\mathtt{PP} = \mathtt{P}^\mathtt{\#P}$.
One indication for how much harder counting is compared to decision problems is \emph{Toda's theorem}~\cite{Toda91}.
It states that $\mathtt{PH} \sub \mathtt{P}^\mathtt{\#P}$ where $\mathtt{PH}$ is the \emph{polynomial hierarchy}, the union of all towers of $\mathtt{NP}$-oracles, i.e. $\mathtt{PH} = \cup_{n \in \N} \Sigma_n$ where $\Sigma_0 = \mathtt{P}$ and $\Sigma_{n + 1} = \mathtt{NP}^{\Sigma_n}$.
Intuitively, a $\mathtt{PP}$-oracle for counting problems is at least as powerful than any tower of $\mathtt{NP}$-oracles for decision problems.

In a beautifully simple theorem, Aaronson~\cite{Aaronson05} shows that $\mathtt{PP} = \mathtt{PostBQP}$, the class of problems solvable in polynomial time by a quantum computer given \emph{post-selection}, the ability to make the possible necessary, i.e. to choose what outcome we get from a quantum measurement\footnote
{Assuming the many-world hypothesis, Aaronson~\cite{Aaronson05} gives a simple method to achieve post-selection: committing suicide if we do not get the desired outcome.
A less brutal method is to keep on trying until we do, in exponential time on average.}.
The related counting class $\mathtt{\#P}$ was originally introduced by Valiant~\cite{Valiant79} to show the completeness the \emph{matrix permanent}, which turns out to be closely related to the complexity of \emph{boson sampling}~\cite{AaronsonArkhipov11}, a candidate task for quantum advantage.
IF DISCOCAT EFFICIENT THEN BOSON SAMPLING SIMULATABLE, THEN POLYNOMIAL HIERARCHY COLLAPSE
Once translated in our setting, we get that evaluating DisCoCat models on an arbitrary discourse (i.e. sentences with arbitrary anaphora) is
