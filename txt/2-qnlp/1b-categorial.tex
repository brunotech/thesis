%!TEX root = ../../THESIS.tex

\subsection{From Lambek to pregroup grammars}

Even if we cannot get rid of the inherent ambiguity of natural language, we can still try to reduce the artificial ambiguity of our grammar formalism, i.e. the number of weakly equivalent grammars that generate the same language.

The \emph{categorial grammar} tradition may be summed up in a slogan: \emph{all the grammar is in the dictionary}~\cite{Preller07}.
Indeed, there is no need for language-specific production rules if the types of our grammar have enough structure, if we go from monoidal to closed categories.
In the \emph{Lambek calculus}~\cite{Lambek58}, a categorial grammar is defined as a tuple $G = (V, X, R, s)$ where:
\begin{itemize}
\item $V$ and $X$ are finite sets called the \emph{vocabulary} and the \emph{basic types} with $s \in X$ the \emph{sentence type},
\item $R \sub V \times T(X)$ is a finite set of \emph{dictionary entries} with $T(X) \supseteq X$ the set of formal expressions with $1, (x \otimes y), (x / y), (x \backslash y) \in T(X)$ for all $x, y \in T(X)$.
\end{itemize}
Equivalently, the Lambek grammar $G$ may be seen as a closed monoidal signature (as defined in section~\ref{subsection:closed}) with dictionary entries as boxes where the domain is a single word\footnote
{Note that the original calculus did not include a unit for the tensor product, here we follow the presentation given by Lambek~\cite{Lambek88} thirty years later.}.
In a \emph{basic categorial grammar}, also called an AB grammar after Ajdukiewicz~\cite{Ajdukiewicz35} and Bar-Hillel~\cite{Bar-Hillel54}, the dictionary is restricted to a closed signature, i.e. types are generated without the tensor product and unit.
The language of a categorial grammar $G$ is given by $L(G) = \{ w \in V^\star \ \vert \ \exists \ f : w \to s \in \G \}$ for $\G$ the free closed category generated by the dictionary.
More explicitly, a grammatical structure $f : w_1 \dots w_n \to s$ is given by a tensor of dictionary entries $(w_i, t_i) \in R$ followed by a closed diagram $t_1 \dots t_n \to s$ composed only of evaluation and currying.
Traditionally, these closed diagrams have been defined in terms of a \emph{sequent calculus} à la Gentzen, see Lambek~\cite{Lambek88} for a translation between the two definitions.

\begin{python}
{\normalfont Implementation of categorial grammars as closed categories.}

\begin{minted}{python}
class Parsing(closed.Diagram, cfg.Parsing): pass
class Ev(closed.Ev, Parsing): pass
class Word(cfg.Word, Parsing): pass
Ev.upgrade = Word.upgrade = Parsing.upgrade
\end{minted}
\end{python}

\begin{example}
We can take $X = \{ s, n, np \}$ and assign common noun the type $n$, determinants $(np \backslash n)$ and transitive verbs $((np / s) \backslash np)$.

\begin{minted}{python}
n, np, s = map(Ty, ('n', 'np', 's'))
man, island = (Word(noun, n) for noun in ("man", "island"))
no, an = (Word(determinant, np << n) for determinant in ("no", "an"))
_is = Word("is", (np >> s) << np)

no_man_is_an_island = no @ man @ _is @ an @ island\
    >> Ev(np << n) @ ((np >> s) << np) @ Ev(np << n)\
    >> np @ Ev((np >> s) << np) >> Ev(np >> s)

no_man_is_an_island.draw()
\end{minted}

\ctikzfig{img/nlp/no-man-is-an-island}
\end{example}

\begin{example}

    TODO: TYPE RAISING
\end{example}

Bar-Hillel et al.~\cite{Bar-HillelEtAl60} showed that basic categorial grammars are strongly equivalent to context-free grammars in \emph{Greibach normal form}~\cite{Greibach65}, where every production has the form $x \to w y$ for a non-terminal $x \in X$, a word $w \in V$ and a string of non-terminals $y \in X^\star$.
Thus, their parsing problem can be solved in polynomial time.
Pentus~\cite{Pentus93} then showed that Lambek grammars are weakly equivalent to CFGs as well, although their parsing problem is $\mathtt{NP}$-complete~\cite{Pentus06}.
This means that unless $\mathtt{P} = \mathtt{NP}$ there are Lambek grammars for which the smallest weakly equivalent CFG will have exponential size.
Many extensions of the Lambek calculus have been introduced to go beyond its context-free limitation and give a more fine-grained description of syntactic phenomena, see Moortgat~\cite{Moortgat14} for a survey.
Additional unary operators called \emph{modalities}\footnote
{There is no consensus on what the definition of modality should be: ``Ask three modal logicians what modal logic is, and you are likely to get at least three different answers''~\cite{BlackburnEtAl02}
In most cases however, modalities are \emph{comonads}~\cite{CirsteaEtAl11}.} allow to break away from the planarity and linearity of closed diagrams, introducing rules for swaps and comonoids in a controlled way to model phenomena such as \emph{parasitic gaps}, \emph{ellipsis} and \emph{anaphora}.
See McPheat et al.~\cite{McPheatEtAl21} where the present author and collaborators introduce a diagrammatic syntax and functorial semantics for such modalities.
The \emph{combinatory categorial grammars} (CCGs) of Steedman~\cite{Steedman87,Steedman00} take a different approach inspired by the combinatory logic of Schönfinkel~\cite{Schonfinkel24} and Curry~\cite{Curry30}, a variable-free predecessor to the lambda-calculus.
In particular, CCGs include \emph{crossed composition} rules which make them mildly context-sensitive, see Kartsaklis and Yeung~\cite{YeungKartsaklis21} for their implementation in DisCoPy.

A key feature of categorial grammars as free closed categories, is that we can define their semantics as functors into any closed category: once the image of dictionary entries is defined, the image of any grammatical structure is fixed.
Montague~\cite{Montague70a,Montague70,Montague73} introduced the idea of semantics as a homomorphism from syntax to logic, i.e. as closed functors $F : \G \to F^{CC}(\Sigma)$ from a categorial grammar into a free cartesian closed category with logical connectives and predicates as boxes.
Although Montague himself did not care much about syntax~\footnote
{``I fail to see any great interest in syntax except as a preliminary to semantics.''~\cite{Montague70a}},
his method provides a general recipe to interpret any categorial grammar in terms of lambda expressions.
From a computational perspective however, Montague semantics is too expressive: as we mentioned in section~\ref{subsection:closed}, the word problem for free cartesian closed categories, or equivalently the normalisation of simply-typed lambda terms, is not elementary recursive.
Even if we manage to translate sentences as first-order logic formulae, the problem of checking if they are valid, the \emph{Entscheidungsproblem} of Hilbert and Ackermann~\cite{HilbertAckerman28}, was proved undecidable by Church~\cite{Church36}.
Intuitively, we can reduce Turing's halting problem to the validity of the sentence ``the machine halts''.

\begin{example}
We can interpret natural language as arbitrary Python code by applying a closed functor $\G \to \mathbf{Pyth}$.

\begin{minted}{python}
n, s = map(Ty, "ns")
program, runs = Word("program", n), Word("runs", n >> s)
program_runs = program @ runs >> Ev(n >> s)

F = closed.Functor(
    dom=Category(Ty, Parsing), cod=Category(list[type], Function),
    ob={n: int, s: int}, ar={program: lambda: 42, runs: lambda: lambda x: x * 10})
assert F(program_runs)() == 420
\end{minted}
\end{example}

Returning to linguistics half a century after his seminal \emph{Mathematics of sentence structure}, Lambek~\cite{Lambek99,Lambek01,Lambek08} introduced \emph{pregroup grammars} as a simplification of his original calculus replacing closed categories by rigid categories.
That is, the dictionary of a pregroup grammar $G = (V, X, R, s)$ has the shape $R \sub V \times (X \times \Z)^\star$, it assigns words to lists of iterated adjoints of basic types.
Again, the language of $G$ is defined as $L(G) = \{ w \in V^\star \ \vert \ \exists \ f : w \to s \in \G \}$ where now $\G$ is the free rigid category generated by the dictionary.
Equivalently, a sentence $w = w_1 \dots w_n \in V^\star$ is grammatical if there are dictionary entries $(w_i, t_i) \in R$ such that $t_1 \dots t_n \leq s$ holds in the free pregroup, i.e. the preorder collapse of $\G$.
In fact, Lambek first defined his pregroup grammars in terms of partial orders (i.e. preorders with antisymmetry) then Preller and he~\cite{PrellerLambek07} reformulated them in terms of free compact 2-categories (i.e. rigid categories with colours) so that they could account for ambiguity.

Every rigid category is also closed, thus for any Lambek grammar $G$ we can construct a pregroup grammar $G'$ and a closed functor $\G \to \G'$ which sends categorial types $x / y$ and $x \backslash y$ to pregroup types $x^r y$ and $x y^l$.
In general this functor need not be faithful: it maps both $(x \otimes y) / z$ and $x \otimes (y / z)$ to the same pregroup type $x y z^l$.
Although they cannot be strongly equivalent to Lambek grammars, Buszkowski~\cite{Buszkowski01} proved that pregroup grammars are context-free, hence they are still weakly-equivalent.
As for the complexity of their parsing problem, Lambek~\cite{Lambek99} first showed it was decidable with the following \emph{switching lemma}: any pregroup inequality $x \leq z$ can be factored into $x \leq y \leq z$ where $x \leq y$ using only cups then $y \leq z$ using only caps.
The proof is essentially given by the snake removal algorithm of listing~\ref{listing:snake-removal} applied to rigid diagrams with only cups and caps: the resulting normal form can be shown to have all cups preceding the caps.
As a corollary, if there is a grammatical structure $f : w_1 \dots w_n \to s$ then there is one using only dictionary entries and cups which we can find by brute force search, Oehrle~\cite{Oehrle04} showed that pregroup parsing can in fact be solved in cubic time.
Preller~\cite{Preller07a} gave sufficient conditions on the dictionary for unambiguous pregroup grammars to be parsed in linear time, the algorithm was later improved and implemented in DisCoPy by Rizzo~\cite{Rizzo21}.

One can define the semantics of a pregroup grammar $G$ as a monoidal functor $F : \G \to C$ into any rigid category $C$, in the \emph{categorical compositional distributional} (DisCoCat) models of Clark, Coecke and Sadrzadeh~\cite{ClarkEtAl08,ClarkEtAl10} one takes $C = \mathbf{Mat}_\C$ the category of complex matrices.
More explicitly, a DisCoCat model $F : \G \to \mathbf{Mat}_\C$ is defined by a dimension $F(x) \in \N$ for every basic type $x \in X$ and a vector $F(w, t) : 1 \to F(t)$ for every dictionary entry $(w, t) \in R$.
It then defines the semantics of a grammatical sentence $f : w_1 \dots w_n \to s$ as the contraction of a tensor network where the nodes are dictionary entries and the edges are given by the cups.
DisCoCat models came out of a quest to accommodate the \emph{compositional} approach to NLP which focused on grammatical structure and the \emph{distributional} apprach that represented words as vectors extracted from text data.
DisCoPy was first meant as an implementation of DisCoCat models before it turned into an implementation of monoidal functors in general.

\begin{python}
{\normalfont Implementation of pregroup grammars as rigid categories.}

\begin{minted}{python}
class Parsing(rigid.Diagram, categorial.Parsing): pass
class Cup(rigid.Cup, Parsing): pass
class Word(categorial.Word, Parsing): pass
Cup.upgrade = Word.upgrade = Parsing.upgrade
\end{minted}
\end{python}

\begin{example}
This was the first example in DisCoPy's documentation.

\begin{minted}{python}
s, n = Ty('s'), Ty('n')
Alice, loves, Bob = Word('Alice', n), Word('loves', n.r @ s @ n.l), Word('Bob', n)
sentence = Alice @ loves @ Bob >> Cup(n, n.r) @ s @ Cup(n.l, n)

sentence.draw()
\end{minted}

\ctikzfig{img/nlp/alice-loves-bob}

\begin{minted}{python}
F = rigid.Functor(
    dom=Category(Ty, Parsing), cod=Category(list[int], Tensor),
    ob={s: 1, n: 2},
    ar={Alice: [[1, 0]], loves: [[0, 1], [1, 0]], Bob: [[0, 1]]})

assert F(sentence)
\end{minted}
\end{example}
