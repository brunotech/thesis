%!TEX root = ../../THESIS.tex

\subsection{Formal grammars, parsing and ambiguity}\label{subsection:chomsky}

The word ``grammar'' comes from the ancient Greek ``$\rm{\gamma \rho\acute{\alpha}\mu\mu\alpha}$'' (line of writing), it is cognate to the words ``glamour'' and ``grimoire'' \cite{RuppliThorel05, Davies10, Lambek14}.
The practice of grammar itself goes back to India somewhere between the 6th and 4th century BCE~\cite{BhateKak93}, where the Sanskrit philologist P\={a}\d{n}ini introduced what was later recognised as \emph{context-sensitive grammars}.
More than two thousand years later, Chomsky~\cite{Chomsky56,Chomsky57} gave grammars their modern definition.
A \emph{formal grammar}, also called \emph{unrestricted} or \emph{type-0} grammar, is a tuple $G = (V, X, R, s)$ where:
\begin{itemize}
    \item $V$ and $X$ are finite sets called \emph{terminal} and \emph{non-terminal} symbols respectively, we will also call them the \emph{vocabulary} and the \emph{basic types},
    \item $R$ is a finite set of \emph{production rules} $x \to y$ where $x, y \in (V + X)^\star$,
    \item $s \in X$ is called the \emph{start symbol} or the \emph{sentence type}.
\end{itemize}
A string of words $w = w_1 \dots w_n \in V^\star$ is a \emph{grammatical sentence} whenever\footnote
{Formal grammars are usually defined in the other direction, i.e. $w \in L(G)$ iff $s \leq_R w$.
We choose the opposite convention so that we won't have to switch in the next section.} $w \leq_R s$
for $(\leq_R)$ the reflexive transitive closure of the rewriting relation as defined in section~\ref{subsection:quotient-categories}.
Thus, the grammar $G$ generates a \emph{language} $L(G) \sub V^\star$, the set of all grammatical sentences.
Although formal grammars are called unrestricted, the right-hand side of the rules in $R$ is usually restricted to be non-empty.
This makes no difference as to the classes of languages that can be generated, i.e. for every grammar with empty right-hand sides there is a grammar without that generates the same language.

Equivalently, a formal grammar is a finite monoidal signature $G$ with an injection from the words in the vocabulary and the sentence type into the generating objects $V + \{ s \} \injects G_0$.
Indeed, we can define the non-terminal symbols as $X = G_0 - V$ then a rewrite rule is nothing but a box with lists of symbols as domain and codomain.
The language of $G$ may then be defined as $L(G) = \{ w \in V^\star \ \vert \ \exists f : w \to s \ \in \G \}$ for $\G$ the free monoidal category generated by $G$, a diagram $f : w \to s$ to the sentence type $s$ is proof that the string of words $w$ is grammatical.
We call the diagram $f : w \to s$ a \emph{grammatical structure} for the sentence $w$, we say a sentence is \emph{ambiguous} whenever it has more than one grammatical structure.
The \emph{parsing problem} is to decide, given a grammar $G$ and a string $w \in V^\star$, whether $w \in L(G)$.
It is easily shown to be equivalent to the word problem for monoids and the halting problem for Turing machines, thus it is undecidable.
Moreover, there exists a \emph{universal grammar} $G$ such that the parsing problem with $G$ fixed and only the string $w \in V^\star$ as input is undecidable.
That is, for any other grammar $G'$ and string $w \in V(G')^\star$ we can compute some other string $w' \in V(G)^\star$ such that $w \in L(G')$ if and only if $w' \in L(G)$.

If we are to build a \emph{parser}, i.e. a machine that computes the grammatical structure of a given string, type-0 grammars are too general: their parsing problem is undecidable.
Going one level up in Chomsky's hierarchy, a \emph{context-sensitive grammar} (CSG, also called a \emph{type-1} grammar) is a formal grammar $G$ where the rules have the form $a b c \to a x c$ for a non-terminal symbol $x \in X$ and lists of symbols $a, b, c \in G_0^\star$ where $\len(b) \geq 1$\footnote
{If we care about whether a language contains the empty string $1$ or not, we also have to allow for the rule $s \to 1$.}.
The parsing problem for CSG was the first to be shown complete for the class $\mathtt{NPSPACE}$ of problems solvable in non-deterministic polynomial space~\cite{Kuroda64}.
Savitch~\cite{Savitch70} then proved $\mathtt{NPSPACE} = \mathtt{PSPACE}$, hence that parsing CSG is in fact complete for deterministic polynomial space.
Another $\mathtt{PSPACE}$-complete problem is the parsing problem for \emph{non-contracting grammars}, where we have that $\len(y) \leq \len(x)$ for every rule $x \to y$.
Indeed, every CSG is also non-contracting and for every non-contracting grammar $G$, there is a CSG $G'$ with $L(G) = L(G')$~\cite[Theorem~11]{Chomsky63}.

Two grammars $G$ and $G'$ over the same vocabulary $V$ are \emph{weakly equivalent} whenever they generate the same language, i.e. $L(G) = L(G') \sub V^\star$.
For example, every non-contracting grammar is weakly-equivalent to a CSG.
A \emph{strong equivalence} preserves not only the generated languages but also the grammatical structure, i.e. it defines a bijection\footnote
{Chomsky~\cite{Chomsky63} defines two grammars to be strongly equivalent when they generate ``the same set of structural descriptions'' but he doesn't define sameness of structural descriptions.
Our definition only asserts that the two grammars assign the same number of grammatical structures to any string, not that these structures are isomorphic themselves.
Asking for an equivalence of monoidal categories $\G \simeq \G'$ would be too strong: when two free categories are equivalent, they are automatically isomorphic.} $\G(w, s) \simeq \G'(f(w), s)$ for all strings $w \in V^\star$.
For example, every CSG is strongly equivalent to a non-contracting grammar (itself) in a trivial way.

For a less trivial example, every formal grammar $G$ is strongly equivalent to a \emph{lexicalised} one, where the rules are a union $R = D \cup R'$ of \emph{dictionary entries} $D \sub V \times X$ assigning possible types to each word and production rules $R' \sub X^\star \times X^\star$ not involving the vocabulary.
Indeed, given a grammar $G$ we can add a new basic type $w'$ and a dictionary entry $w \to w'$ for each word $w \in V$ to get a lexicalised grammar $G'$.
Every grammatical structure $f : w_1 \dots w_n \to s$ in $\G'$ factorises as $f = d \fcmp f'$ for a tensor of dictionary entries $d : w_1 \dots w_n \to w_1' \dots w_n'$ and a diagram $f' : w_1' \dots w_n' \to s$ with no dictionary entries, which is isomorphic to a grammatical structure in $\G$.
Once the grammar is lexicalised, we usually draw dictionary entries as boxes labeled by the corresponding word and we omit the wires for terminal symbols.
We also assume that any \emph{semantic functor} $F : \G \to C$ from a lexicalised grammar $\G$ to some concrete category $C$ maps dictionary entries to states, i.e. $F(w) = 1$ for all words $w \in V$, which implies that the interpretation of any grammatical structure is a also state $F(f) : 1 \to F(s)$.

Unless $\mathtt{P} = \mathtt{NP} = \mathtt{PSPACE}$, there can be no efficient parser for context-sensitive grammars in general.
This motivates the introduction of \emph{context-free grammars} (CFGs, also called \emph{type-2} grammars) where the right-hand side of each rule has length one.
We can assume that the grammar is lexicalised, so that the rules have the form either $w \to x$ or $y_1 \dots y_n \to x$ for a basic type $x \in X$, a word $w \in V$ and a list of basic types $y_1 \dots y_n \in X^\star$.
In this case, grammatical structures $f : w_1 \dots w_n \to s$ have the shape of a \emph{syntax tree} with the words $w_1 \dots w_n$ as leaves and the sentence type $s$ as root.
The interchanger normal form of a syntax tree is called its \emph{left-most derivation}, when two rules apply in parallel the left-most is always applied first.
A CFG is in \emph{Chomsky normal form} (CNF\footnote
{CNF is \emph{not} a normal form in the sense that it computes representatives of equivalence classes, a given grammar may have many non-isomorphic CNFs.
In fact, deciding whether two CFGs are weakly equivalent is undecidable~\cite[Theorem~26]{Chomsky63}.}) when it is lexicalised and the rules are of the form either $s \to 1$ or $x y \to z$ for $x, y \in X - \{ s \}$ qnd $z \in X$, i.e. where all the syntax trees are binary and the sentence type appears only at the root.
Every context-free grammar $\G$ can be converted to some weakly equivalent $\G'$ in CNF, with at most a quadratic blow-up in size.
There is a monoidal functor $\G \to \G'$ mapping every $n$-ary rule to a tree of $n - 1$ binary rules when $n \geq 2$ and to the identity when $n < 2$.
This means that \emph{nullable} types, i.e. from which we can derive the empty string, are all sent to the monoidal unit.
Thus in the presence of unary and nullary rules, the functor cannot be faithful and the equivalence cannot be strong.

The CYK (Cocke–Younger–Kasami) algorithm solves the parsing problem for CNF in cubic time using dynamic programming.
Valiant~\cite{Valiant75} then reduced the problem to Boolean matrix multiplication, yielding a solution in time $O(n^{\log_2 7})$ via Strassen's algorithm.
Today, the fastest algorithm known for matrix multiplication, hence for parsing context-free grammars, is the galactic algorithm by Alman and Williams~\cite{AlmanWilliams21}.
Parsing context-free grammars is in fact complete for $\mathtt{P}$, the class of problems solvable in deterministic polynomial time~\cite{JonesLaaser74}.
Hence, whatever grammatical framework we may come up with, if its parsing problem is solvable in polynomial time then there exists a logarithmic-space reduction to CFG parsing: it takes a grammar and a string, returns a CFG and a new string such that the input is grammatical if and only if the output is.
Crucially, the output CFG depends not only on the input grammar but also on the input string: $\mathtt{P}$-completeness does not imply that there exists one fixed CFG that generates the same language.
This opens the door to grammars that are more expressive than context-free but still efficiently-parsable.

Indeed, there is evidence for some degree context-sensitivity in natural language~\cite{Huybregts84,Shieber85}.
The most studied examples are the \emph{cross-serial dependencies} of Dutch and Swiss German, which have been abstracted as the formal language $\{ w^k \ \vert \ w \in V^\star, k \leq n \}$ for some (low) constant threshold $k \leq n$.
Thus, several \emph{mildly context-sensitive grammar} (MCSG) formalisms have been introduced, which generate all of the context-free languages as well as cross-serial dependencies, yet are still parsable in polynomial time.
All MCSGs proposed so far have fallen into one of three classes of weak equivalence~\cite{Weir88}.
Thus, there is reasonable consensus over the kind of computational power required to parse human language, at least up to weak equivalence, see Kallmeyer~\cite{Kallmeyer10} for a standard survey.
However, there is no consensus yet on the syntactic way this computational power should be expressed: apart from some isolated results~\cite{SchifferMaletti21}, there is no classification of MCSGs up to strong equivalence.

Whether two grammars are strongly equivalent matters when we want to define their semantics, i.e. we want to compute the interpretation of a sentence given its grammatical structure.
Indeed, weakly equivalent grammars may assign different sets of possible parsing to the same ambiguous sentence, which will correspond to different interpretations.
For example, we can apply a monoidal functor from a CFG to a category of neural networks, which yields a \emph{recursive neural network} that computes the meaning of a sentence given its parse tree~\cite{SocherEtAl11,SocherEtAl13}.
Different trees will result in different network architectures, so how do we know we have picked the right one?
We can use a \emph{probabilistic grammar}~\cite{Salomaa69} to compute the most likely grammatical structure given some training data, in some cases with theoretical guarantees that this is indeed learnable efficiently~\cite{ClarkEtAl06,ShibataYoshinaka16}.

DisCoPy implements formal grammars with \py{Parsing}, a subclass of \py{Diagram} with \py{Word} and \py{Production} as boxes.
It does not implement any parsing algorithm, however it is straightforward to encode the output of an existing parser e.g. that of NLTK~\cite{LoperBird02} into a \py{Parsing} diagram so that we can compute the semantics of sentences by applying a \py{Functor}.

\begin{python}
{\normalfont Implementation of the \py{grammar} module and its interface with NLTK.}

\begin{minted}{python}
class Parsing(monoidal.Diagram):
    @staticmethod
    def fromtree(tree: nltk.Tree) -> Parsing:
        if len(tree) == 1 and isinstance(tree[0], str):
            return Word(tree[0], Ty(tree.label()))
        subtrees = Parsing.tensor(*[Parsing.fromtree(t) for t in tree])
        return subtrees >> Production(dom=subtrees.cod, cod=Ty(tree.label()))

class Word(monoidal.Box, Parsing):
    def __init__(self, name: str, cod: Ty, dom=Ty()):
        monoidal.Box.__init__(self, name, dom, cod)

class Production(monoidal.Box, Parsing):
    def __init__(self, dom: Ty, cod: Ty):
        name = "Production({}, {})".format(dom, cod)
        monoidal.Box.__init__(self, name, dom, cod)

Word.cast = Production.cast = Parsing.cast
\end{minted}
\end{python}

\begin{example}
We use the recursive descent parser from NLTK to parse an ambiguous expression and draw its possible parsings.

\begin{minted}{python}
from nltk import CFG, BottomUpChartParser as Parser

grammar = """
n -> a n
n -> n n
a -> 'black'
a -> 'metal'
n -> 'metal'
n -> 'fan'
"""
parser = Parser(CFG.fromstring(grammar)).parse

for tree in parser("black metal fan".split()): Parsing.fromtree(tree).draw()
\end{minted}
\begin{center}
\tikzfig{img/nlp/black-metal-fan/0}
\hfill
\tikzfig{img/nlp/black-metal-fan/1}
\hfill
\tikzfig{img/nlp/black-metal-fan/2}
\end{center}
If we fed these syntax trees as input to the recursive neural network of Socher et al.~\cite{SocherEtAl11} (which was trained to generate images from text descriptions) we would expect to get the following images as output:
\begin{center}
\includegraphics[width=0.25\textwidth]{img/nlp/black-metal-fan/0.png}
\hfill
\includegraphics[width=0.25\textwidth]{img/nlp/black-metal-fan/1.jpeg}
\hfill
\includegraphics[width=0.25\textwidth]{img/nlp/black-metal-fan/2.jpeg}
\end{center}
\end{example}

In the framework of context-free grammars, ambiguity arises in at least two ways: 1) we have to choose from the many weakly equivalent grammars that generate the same language, 2) once the grammar is fixed we have to choose from the many syntax trees that generate the same sentence.
The second type of ambiguity cannot be alleviated: Parikh~\cite{Parikh61,Parikh66} defined a context-free language that is \emph{inherently ambiguous} in the sense that no unambiguous grammar can generate it~\cite[Theorem~29]{Chomsky63}.
In the same paper, Parikh unravels a deep connection between the theory of context-free grammars and that of free rigs: CFGs $G$ with symbols $G_0$ are in one to one corresondance with endomorphisms of the free rig $f_G : \N[G_0] \to \N[G_0]$ which fix the vocabulary, i.e. $f_G(w) = w$ for all words $w \in V$~\cite[Section~3]{Parikh66}.
Indeed, the free rig $\N[X]$ generated by a set $X$ has underlying set $\N^{X^\star}$, it can be thought of as the set of \emph{languages with multiplicities}.
Iterating the endomorphism $n$ times then projecting on the vocabulary with $\pi_V : \N[G_0] \to \N[V]$, the formal sum $\pi_V(f_G^n(s)) \in \N[G_0]$ has a term for each grammatical sentence that can be generated by a syntax tree of depth $n$ and the coefficients given by the ambiguity of the sentence, i.e. the number of different syntax trees~\cite{MichaelisKracht97}.
Thus we can define the language of a CFG as $L(G) = \cup_{n \in \N} \mathtt{sign}(\pi_V(f_G^n(s)))$ for $\mathtt{sign} : \N[V] \to \B[V] \simeq \B^{V^\star}$ the quotient map induced by $1 + 1 = 1$, i.e. forgetting multiplicities.

Parikh~\cite[Theorem~2]{Parikh66} states that if we define the map $p : V^\star \to \N^V$ which sends lists to bags by forgetting word order, then the direct image $p(L(G)) \sub \N^V$ is indistinguishable from that of a \emph{regular grammar}.
Regular grammars (also called \emph{type-3}) have rules of the form either $x \to 1$ or $x \to w y$ for non-terminals $x, y \in X$ and word $w \in V$, they are the least expressive level in Chomsky's hierarchy.
The subsets $p(L(G)) \sub \N^V$ generated by CFGs (or equivalently by regular grammars) are called \emph{semilinear}, they are finite unions of affine subspaces\footnote
{A subspace of $\N^V$ is affine if it has the form $\{ u_0 + t_1 u_1 + \dots + t_n u_n \ \vert \ t_1, \dots, t_n \in \N \}$ for some $u_0, \dots u_n \in \N^V$.
Confusingly, affine subspaces are called ``linear'' in the literature, hence ``semilinear''.
}.
Semilinearity is sometimes required as an extra condition for a grammar to be considered mildly context-sensitive, although there is evidence that some natural languages like Old Georgian are not semilinear~\cite{MichaelisKracht97}.
Regular grammars can be be equivalently defined as \emph{regular expressions}: elements of the free \emph{Kleene algebra} $K(V)$, the free idempotent rig with a closure\footnote
{A closure is an idempotent monad on a preorder, here given by $a \leq b$ iff $\exists c \cdot a + c = b$.} $(-)^\star : K(V) \to K(V)$.
Equality of regular expressions is decidable, hence so is the weak equivalence of regular grammars.
Moreover, every regular language can be generated unambiguously.
Thus, Parikh's theorem tells us intuitively that the hardness of natural language comes from its non-commutativity: if we forget about word order then everything is decidable and ambiguity disappears.
From our applied category theory perspective, this also means that language cannot be fully\footnote
{That is, faithful functors from non-regular CFGs to symmetric categories cannot be full.} investigated in symmetric categories, we need a planar monoidal data structure such as DisCoPy's \py{Diagram}.
