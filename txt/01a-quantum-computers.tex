%!TEX root = ../THESIS.tex

\section*{What are quantum computers good for?}
\addcontentsline{toc}{section}{What are quantum computers good for?}

\justepigraph{
Nature isn't classical, dammit, and if you want to make a simulation of nature, you'd better make it quantum mechanical, and by golly it's a wonderful problem, because it doesn't look so easy.
}{\textit{Simulating Physics with Computers}, Feynman (1981)}

Quantum computers harness the principles of quantum theory such as superposition and entanglement to solve information-processing tasks.
In the last 42 years, quantum computing has gone from theoretical speculations to the implementation of machines that can solve problems beyond what is possible with classical means.
This section will sketch a brief and biased history of the field and of its future challenges.

In 1980, Benioff~\cite{Benioff80} takes the abstract definition of a computer and makes it physical: he designs a quantum mechanical system whose time evolution encodes the computation steps of a given Turing machine.
In retrospect, this may be taken as the first proof that quantum mechanics can simulate classical computers.
The same year, Manin~\cite{Manin80} looks at the opposite direction: he argues that it would take exponential time for a classical computer to simulate a generic quantum system.
Feynman~\cite{Feynman82,Feynman85} comes to the same conclusion and suggests a way to simulate quantum mechanics much more efficiently: building a quantum computer!

So what are quantum computers good for? Feynman's intuition gives us a first, trivial answer: at least quantum computers could simulate quantum mechanics efficiently. Deutsch~\cite{Deutsch85a} makes the question formal by defining quantum Turing machines and the circuit model.
Deutsch and Jozsa~\cite{DeutschJozsa92} design the first quantum algorithm and prove that it solves \emph{some} problem exponentially faster than any classical \emph{deterministic} algorithm.\footnote
{A classical \emph{randomised} algorithm solves the problem in constant time with high probability.}
Simon~\cite{Simon94} improves on their result by designing a problem that a quantum computer can solve exponentially faster than any classical algorithm.
Deutsch-Jozsa and Simon relied on oracles\footnote{An oracle is a black box that allows a Turing machine to solve a certain problem in one step.}
and promises\footnote{The input is promised to satisfy a certain property, which may be hard to check.} and their problems have little practical use.
However, they inspired Shor's algorithm~\cite{Shor94} for prime factorisation and discrete logarithm. These two problems are believed to require exponential time for a classical computer and their hardness is at the basis of the public-key cryptography schemes currently used on the internet.

In 1997, Grover provides another application for quantum computers: ``searching for a needle in a haystack''~\cite{Grover97}.
Formally, given a function $f : X \to \{0, 1\}$ and the promise that there is a unique $x \in X$ with $f(x) = 1$, Grover's algorithm finds $x$ in $O(\sqrt{|X|})$ steps, quadratically faster than the optimal $O(|X|)$ classical algorithm.
Grover's algorithm may be used to brute-force symmetric cryptographic keys twice bigger than what is possible classically~\cite{BernsteinEtAl09}.
It can also be used to obtain quadratic speedups for the exhaustive search involved in the solution of NP-hard problems such as constraint satisfaction~\cite{Ambainis04}.
Independently, Benett et al.~\cite{BennettEtAl97} prove that Grover's algorithm is in fact optimal, adding evidence to the conjecture that quantum computers cannot solve these NP-hard problems in polynomial time.
Chuang et al.~\cite{ChuangEtAl98} give the first experimental demonstration of a quantum algorithm, running Grover's algorithm on two qubits.

Shor's and Grover's discovery of the first real-world applications sparked a considerable interest in quantum computing.
The core of these two algorithms has then been abstracted away in terms of two
subroutines: phase estimation~\cite{Kitaev95} and amplitude
amplification~\cite{BrassardEtAl02}, respectively.
Making use of both these subroutines, the HHL\footnote{Named after its discoverers Harrow, Hassidim and Lloyd.} algorithm~\cite{HarrowEtAl09} tackles one of the most ubiquitous problems in scientific computing: solving systems of linear equations.
Given a matrix $A \in \mathbb{R}^{n \times n}$ and a vector ${b} \in \mathbb{R}^{n}$, we want to find a vector ${x}$ such that $A {x} = {b}$.
Under some assumptions on the sparsity and the condition number of $A$, HHL finds (an approximation of) $x$ in time logarithmic in $n$ when a classical algorithm would take quadratic time simply to read the entries of $A$.
This initiated a new wave of enthusiasm for quantum computing with the promise of exponential speedups for machine learning tasks such as regression~\cite{WiebeEtAl12}, clustering~\cite{LloydEtAl13}, classification~\cite{RebentrostEtAl14}, dimensionality reduction~\cite{LloydEtAl14} and recommendation~\cite{KerenidisPrakash16}.
The narrative is appealing: machine learning is about finding patterns in large amounts of data represented as high-dimensional vectors and tensors, which is precisely what quantum computers are good at.
The argument can be formalised in terms of complexity theory: HHL is \texttt{BQP}-complete\footnote
{A \texttt{BQP}-complete problem is one that is polynomial-time  equivalent to the circuit model, the hardest problem that a quantum computer can solve with bounded error in polynomial time.}
hence if there is an exponential advantage for quantum algorithms at all there must be one for HHL.

However, the exponential speedup of HHL comes with some caveats, thoroughly analysed by Aaronson~\cite{Aaronson15}.
Two of these challenges are common to many quantum algorithms:
1) the efficient encoding of classical data into quantum states and
2) the efficient extraction of classical data via quantum measurements.
Indeed, what HHL really takes as input is not a vector ${b}$ but a quantum state $\ket{b} = \sum_{i=1}^n b_i \ket{i}$ called its amplitude encoding.
Either the input vector ${b}$ has enough structure that we can describe it with a simple, explicit formula.
This is the case for example in the calculation of electromagnetic scattering cross-sections~\cite{CladerEtAl13}.
Or we assume that our classical data has been loaded onto a quantum random-access memory (qRAM) that can prepare the state in logarithmic time~\cite{GiovannettiEtAl08}.
Not only is qRAM a daunting challenge from an engineering point of view, in some cases it also requires too much error correction for the state preparation to be efficient \cite{ArunachalamEtAl15}.
Symmetrically, the output of HHL is not the solution vector ${x}$ itself but a quantum state $\ket{x}$ from which we can measure some observable $\bra{x} M \ket{x}$.
If preparing the state $\ket{b}$ requires a number of gates exponential in the number of qubits, or if we need exponentially many measurements of $\ket{x}$ to compute our classical output, then the quantum speedup disappears.

Shor, Grover and HHL all assume \emph{fault-tolerant} quantum computers~\cite{Shor96}.
Indeed, any machine we can build will be subject to noise when performing quantum operations, errors are inevitable: we need an error correcting code that can correct these errors faster than they appear.
This is the content of the \emph{quantum threshold theorem}~\cite{AharonovBen-Or08} which proves the possibility of fault-tolerant quantum computing given physical error rates below a certain threshold.
One noteworthy example of such a quantum error correction scheme is Kitaev's toric code~\cite{Kitaev03} and the general idea of topological quantum computation~\cite{FreedmanEtAl03} which offers the long-term hope for a quantum computer that is fault-tolerant ``by its physical nature''.
However this hope relies on the existence of quasi-particles called Majorana zero-modes, which as of 2021 has yet to be experimentally demonstrated~\cite{Ball21}.

The road to large-scale fault-tolerant quantum computing will most likely be a long one.
So in the meantime, what can we do with the noisy intermediate-scale quantum machines we have available today, in the so-called NISQ era~\cite{Preskill18}?
Most answers involve a hybrid classical-quantum approach where a classical algorithm is used to optimise the preparation of quantum states~\cite{McCleanEtAl16}.
Prominent examples include the quantum approximate optimisation algorithm (QAOA~\cite{FarhiEtAl14}) for combinatorial problems such as maximum cut and the variational quantum eigensolver (VQE~\cite{PeruzzoEtAl14}) for approximating the ground state of chemical systems.
These variational algorithms depend on the choice of a parameterised quantum circuit called the \emph{ansatz}, based on the structure of the problem and the resources available.
Some families of ans√§tze such as instantaneous quantum polynomial-time (IQP) circuits are believed to be hard to simulate classically even at constant depth \cite{ShepherdBremner09}, opening the door to potentially near-term NISQ speedups.

Although the hybrid approach first appeared in the context of machine learning~\cite{BangEtAl08}, the idea of using parameterised quantum circuits as machine learning models went mostly unnoticed for a decade~\cite{BenedettiEtAl19}.
It was rediscovered under the name of quantum neural networks~\cite{FarhiNeven18} then implemented on two-qubits~\cite{HavlicekEtAl19}, generating a new wave of attention for quantum machine learning.
The idea is straightforward: 1) encode the input vector ${x} \in \mathbb{R}^n$ as a quantum state $\ket{\phi_{x}}$ via the ansatz of our choice, 2) initialise a random vector of parameters ${\theta} \in \mathbb{R}^d$ and encode it as a measurement $M_\theta$, again via some choice of ansatz 3) take the probability $y = \bra{\phi({x})} M_\theta \ket{\phi({x})}$ as the prediction of the model.
A classical algorithm then uses this quantum prediction as a subroutine to find the optimal parameters $\theta$ in some data-driven task such as classification.

One of the many challenges on the way to solving real-world problems with parameterised quantum circuits is the existence of \emph{barren plateaus}~\cite{McCleanEtAl18}:
with random circuits as ansatz, the probability of non-zero gradients is exponentially small in the number of qubits and our classical optimisation gets lost in a flat landscape.
One can help but notice the striking similarity with the vanishing gradient
problem for classical neural networks, formulated twenty years earlier~\cite{Hochreiter98}.
Barren plateaus do not appear in circuits with enough structure such as quantum convolutional networks~\cite{PesahEtAl21}, they can also be mitigated by structured initialisation strategies~\cite{GrantEtAl19}.
Another direction is to avoid gradients altogether and use \emph{kernel methods}~\cite{SchuldKilloran19}:
instead of learning a measurement $M_\theta$, we use our NISQ device to estimate the distance $|\braket{\phi_{x'}}{\phi_x}|^2$ between pairs of input vectors $x, x' \in \mathbb{R}^n$ embedded in the high-dimensional Hilbert space of our ansatz.
We then use a classical support vector machine to find the optimal hyperplane that separates our data, with theoretical guarantees to learn quantum models at least as good as the variational approach~\cite{Schuld21}.

Random quantum circuits may be unsuitable for machine learning, but they play a crucial role in the quest for \emph{quantum advantage}, the experimental demonstration of a quantum computer solving a task that cannot be solved by classical means in any reasonable time.
We are back to Feynman's original intuition: sampling from a random quantum circuit is the perfect candidate for such a task.
The end of 2019 saw the first claim of such an advantage with a 53-qubit computer~\cite{AruteEtAl19}.
The claim was almost immediately contested by a classical simulation of 54 qubits in two and a half days~\cite{PednaultEtAl19} then in five minutes~\cite{YongEtAl21}.
Zhong et al.~\cite{ZhongEtAl20} made a new claim with a 76-photon linear optical quantum computer followed by another with a 66-qubit computer~\cite{WuEtAl21,ZhuEtAl21}.
They estimate that a classical simulation of the sampling task they completed in a couple of hours would take at least ten thousand years.

Now that quantum computers are being demonstrated to compute something beyond classical, the question remains: can they compute something \emph{useful}?
